{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "88a6452d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import snscrape.modules.twitter as sntwitter\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import pprint\n",
    "\n",
    "\n",
    "def _to_date(d):\n",
    "    \"\"\"Normalize input to a date object.\n",
    "\n",
    "    Accepts a string in ISO format 'YYYY-MM-DD', a datetime, or a date\n",
    "    and returns a datetime.date.\n",
    "    \"\"\"\n",
    "    from datetime import date as _date\n",
    "\n",
    "    if isinstance(d, str):\n",
    "        # Try ISO format first (YYYY-MM-DD)\n",
    "        try:\n",
    "            return datetime.fromisoformat(d).date()\n",
    "        except Exception:\n",
    "            # Fallback to common format\n",
    "            return datetime.strptime(d, \"%Y-%m-%d\").date()\n",
    "    if isinstance(d, datetime):\n",
    "        return d.date()\n",
    "    if isinstance(d, _date):\n",
    "        return d\n",
    "    raise TypeError(f\"Unsupported date type: {type(d)}\")\n",
    "\n",
    "\n",
    "def _parse_timestamp(item):\n",
    "    \"\"\"Return a datetime (or None) from several possible fields in a yfinance news item.\"\"\"\n",
    "    ts = item.get('providerPublishTime')\n",
    "    if ts:\n",
    "        try:\n",
    "            ts = int(ts)\n",
    "            if ts > 1e12:\n",
    "                return datetime.fromtimestamp(ts / 1000.0)\n",
    "            else:\n",
    "                return datetime.fromtimestamp(ts)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    for key in ('pubDate',):\n",
    "        val = item.get(key) or (item.get('content') or {}).get(key)\n",
    "        if val and isinstance(val, str):\n",
    "            try:\n",
    "                s = val.rstrip('Z')\n",
    "                return datetime.fromisoformat(s)\n",
    "            except Exception:\n",
    "                try:\n",
    "                    return datetime.strptime(val, '%Y-%m-%dT%H:%M:%SZ')\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "    val = (item.get('content') or {}).get('displayTime')\n",
    "    if val and isinstance(val, str):\n",
    "        try:\n",
    "            s = val.rstrip('Z')\n",
    "            return datetime.fromisoformat(s)\n",
    "        except Exception:\n",
    "            try:\n",
    "                return datetime.strptime(val, '%Y-%m-%dT%H:%M:%SZ')\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def get_stock_news(ticker, start_date, end_date, verbose=False, include_latest_if_empty=False, latest_n=10):\n",
    "    \"\"\"Get news headlines for a stock using yfinance with robust date parsing.\n",
    "\n",
    "    If include_latest_if_empty is True and no items fall in the requested\n",
    "    date range, return up to `latest_n` most-recent items from yfinance.news\n",
    "    (useful when yfinance only has very recent items outside the requested\n",
    "    range).\n",
    "    \"\"\"\n",
    "    stock = yf.Ticker(ticker)\n",
    "    news_out = []\n",
    "\n",
    "    sd = _to_date(start_date)\n",
    "    ed = _to_date(end_date)\n",
    "\n",
    "    if verbose:\n",
    "        print('fetched items:', len(stock.news))\n",
    "\n",
    "    parsed_items = []\n",
    "    # First pass: parse timestamps for all items and collect those in range\n",
    "    for item in stock.news:\n",
    "        try:\n",
    "            pub_dt = _parse_timestamp(item)\n",
    "            if pub_dt is None:\n",
    "                if verbose:\n",
    "                    print('could not parse timestamp for item; keys:', list(item.keys()))\n",
    "                continue\n",
    "            parsed_items.append((pub_dt, item))\n",
    "            pub_date_only = pub_dt.date()\n",
    "            if sd <= pub_date_only <= ed:\n",
    "                content_block = item.get('content') or {}\n",
    "                title = item.get('title') or content_block.get('title') or content_block.get('headline') or ''\n",
    "                content_text = item.get('content') or content_block.get('summary') or content_block.get('description') or ''\n",
    "                news_out.append({\n",
    "                    'date': pub_date_only,\n",
    "                    'title': title,\n",
    "                    'content': content_text\n",
    "                })\n",
    "        except Exception as e:\n",
    "            if verbose:\n",
    "                print('error processing news item:', e)\n",
    "            continue\n",
    "\n",
    "    # If nothing matched and user asked for latest items, return most-recent parsed items\n",
    "    if len(news_out) == 0 and include_latest_if_empty and parsed_items:\n",
    "        if verbose:\n",
    "            print('No items in range — returning latest', latest_n, 'items instead')\n",
    "        # sort parsed items by datetime desc and take latest_n\n",
    "        parsed_items.sort(key=lambda x: x[0], reverse=True)\n",
    "        for pub_dt, item in parsed_items[:latest_n]:\n",
    "            pub_date_only = pub_dt.date()\n",
    "            content_block = item.get('content') or {}\n",
    "            title = item.get('title') or content_block.get('title') or content_block.get('headline') or ''\n",
    "            content_text = item.get('content') or content_block.get('summary') or content_block.get('description') or ''\n",
    "            news_out.append({\n",
    "                'date': pub_date_only,\n",
    "                'title': title,\n",
    "                'content': content_text\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(news_out)\n",
    "\n",
    "\n",
    "def get_twitter_posts(query, start_date, end_date, max_tweets=1000):\n",
    "    \"\"\"Get tweets mentioning a stock ticker\"\"\"\n",
    "    tweets = []\n",
    "    # Build search strings. Accepts either strings or date/datetime objects.\n",
    "    if isinstance(start_date, (str,)):\n",
    "        start_str = start_date\n",
    "    else:\n",
    "        start_str = _to_date(start_date).isoformat()\n",
    "\n",
    "    if isinstance(end_date, (str,)):\n",
    "        end_str = end_date\n",
    "    else:\n",
    "        end_str = _to_date(end_date).isoformat()\n",
    "\n",
    "    search_query = f\"${query} since:{start_str} until:{end_str}\"\n",
    "    \n",
    "    for i, tweet in enumerate(sntwitter.TwitterSearchScraper(search_query).get_items()):\n",
    "        if i >= max_tweets:\n",
    "            break\n",
    "        tweets.append({\n",
    "            'date': tweet.date.date(),\n",
    "            'content': getattr(tweet, 'rawContent', getattr(tweet, 'content', '')),\n",
    "            'url': getattr(tweet, 'url', None)\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(tweets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "510721d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fetched items: 10\n",
      "Collected: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df = get_stock_news('AAPL', '2020-01-01', '2025-10-01', verbose=True)\n",
    "print('Collected:', len(news_df))\n",
    "news_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4c5634",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481b218b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "67b4f4a2",
   "metadata": {},
   "source": [
    "# News sources added\n",
    "\n",
    "This notebook now includes two alternative methods to fetch historical news for a ticker:\n",
    "\n",
    "1. NewsAPI (recommended) — reliable and paginated, requires an API key. Use `fetch_news_newsapi(...)`.\n",
    "2. Yahoo Finance HTML scraper (best-effort) — no API key but fragile and may miss or break over time. Use `fetch_news_yahoo(...)`.\n",
    "\n",
    "Run the appropriate cell(s) below and provide an API key for the NewsAPI cell if you choose that option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6c983bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NewsAPI fetcher (replace YOUR_API_KEY)\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "def fetch_news_newsapi(q, from_date, to_date, api_key, page_size=100, max_pages=10, verbose=True):\n",
    "    \"\"\"Fetch news via NewsAPI (https://newsapi.org/). Returns a DataFrame.\n",
    "\n",
    "    - q: query string (e.g., 'AAPL OR Apple')\n",
    "    - from_date/to_date: 'YYYY-MM-DD' strings\n",
    "    - api_key: your NewsAPI key\n",
    "    \"\"\"\n",
    "    url = \"https://newsapi.org/v2/everything\"\n",
    "    page = 1\n",
    "    all_articles = []\n",
    "    while page <= max_pages:\n",
    "        params = {\n",
    "            'q': q,\n",
    "            'from': from_date,\n",
    "            'to': to_date,\n",
    "            'pageSize': page_size,\n",
    "            'page': page,\n",
    "            'apiKey': api_key,\n",
    "            'language': 'en'\n",
    "        }\n",
    "        r = requests.get(url, params=params, timeout=30)\n",
    "        r.raise_for_status()\n",
    "        data = r.json()\n",
    "        articles = data.get('articles', [])\n",
    "        if verbose:\n",
    "            print(f\"page {page}: got {len(articles)} articles\")\n",
    "        if not articles:\n",
    "            break\n",
    "        for a in articles:\n",
    "            published = a.get('publishedAt')\n",
    "            try:\n",
    "                if published:\n",
    "                    dt = datetime.fromisoformat(published.rstrip('Z'))\n",
    "                else:\n",
    "                    dt = None\n",
    "            except Exception:\n",
    "                dt = None\n",
    "            all_articles.append({\n",
    "                'date': dt.date() if dt else None,\n",
    "                'title': a.get('title'),\n",
    "                'content': a.get('content') or a.get('description'),\n",
    "                'source': a.get('source', {}).get('name'),\n",
    "                'url': a.get('url')\n",
    "            })\n",
    "        total = data.get('totalResults')\n",
    "        # stop if we've collected all or if fewer than page_size returned\n",
    "        if total and len(all_articles) >= total:\n",
    "            break\n",
    "        if len(articles) < page_size:\n",
    "            break\n",
    "        page += 1\n",
    "    return pd.DataFrame(all_articles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2ac3315a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yahoo Finance HTML scraper (best-effort)\n",
    "# NOTE: fragile; Yahoo may change their layout. Use responsibly and respect robots.txt/TOS.\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "HEADERS = {\"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0 Safari/537.36\"}\n",
    "\n",
    "def fetch_news_yahoo(ticker, max_pages=3, verbose=True, session=None):\n",
    "    \"\"\"Scrape news items from Yahoo Finance quote/news pages (best-effort).\n",
    "\n",
    "    This function tries several common Yahoo URLs and falls back to the quote page if the dedicated news page returns 404.\n",
    "    It uses a requests.Session with a browser-like User-Agent and de-duplicates results.\n",
    "    Returns a DataFrame with columns ['title','url'] or an empty DataFrame if nothing is found.\n",
    "    \"\"\"\n",
    "    if session is None:\n",
    "        session = requests.Session()\n",
    "    session.headers.update(HEADERS)\n",
    "\n",
    "    # Try a few URL formats that Yahoo has used\n",
    "    tried_urls = [\n",
    "        f\"https://finance.yahoo.com/quote/{ticker}/news?p={ticker}\",\n",
    "        f\"https://finance.yahoo.com/quote/{ticker}/news\",\n",
    "        f\"https://finance.yahoo.com/quote/{ticker}\",\n",
    "    ]\n",
    "\n",
    "    articles = []\n",
    "    for url in tried_urls:\n",
    "        try:\n",
    "            if verbose:\n",
    "                print('fetching', url)\n",
    "            r = session.get(url, timeout=20)\n",
    "            # If we get a 404 for one of the common shapes, try the next URL instead of failing\n",
    "            if r.status_code == 404:\n",
    "                if verbose:\n",
    "                    print('404 for', url)\n",
    "                continue\n",
    "            r.raise_for_status()\n",
    "            soup = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "            items = []\n",
    "            # Common selectors: list items that contain stream news, h3 anchors, and links that include '/news/'\n",
    "            for node in soup.select('li.js-stream-content h3 a, h3 a, a[href*=\"/news/\"]'):\n",
    "                title = node.get_text(strip=True)\n",
    "                href = node.get('href')\n",
    "                if not href:\n",
    "                    continue\n",
    "                if href.startswith('/'):\n",
    "                    href = 'https://finance.yahoo.com' + href\n",
    "                items.append((title, href))\n",
    "\n",
    "            # Fallback: look for <article> tags or long anchor text that looks like headlines\n",
    "            if not items:\n",
    "                for article in soup.find_all('article'):\n",
    "                    a = article.find('a')\n",
    "                    if not a:\n",
    "                        continue\n",
    "                    title = a.get_text(strip=True)\n",
    "                    href = a.get('href')\n",
    "                    if href and href.startswith('/'):\n",
    "                        href = 'https://finance.yahoo.com' + href\n",
    "                    items.append((title, href))\n",
    "\n",
    "            # Final fallback: any long anchor with a '/news/' path\n",
    "            if not items:\n",
    "                for a in soup.select('a'):\n",
    "                    href = a.get('href') or ''\n",
    "                    txt = a.get_text(strip=True)\n",
    "                    if '/news/' in href and txt:\n",
    "                        if href.startswith('/'):\n",
    "                            href = 'https://finance.yahoo.com' + href\n",
    "                        items.append((txt, href))\n",
    "\n",
    "            # De-duplicate while preserving order\n",
    "            seen = set()\n",
    "            for title, href in items:\n",
    "                if not href or href in seen:\n",
    "                    continue\n",
    "                seen.add(href)\n",
    "                articles.append({'title': title, 'url': href})\n",
    "\n",
    "            # If we found any articles on this page, stop trying other URL shapes\n",
    "            if articles:\n",
    "                break\n",
    "        except requests.exceptions.HTTPError as e:\n",
    "            if verbose:\n",
    "                print('HTTP error while fetching', url, e)\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            if verbose:\n",
    "                print('Error fetching/parsing', url, e)\n",
    "            continue\n",
    "\n",
    "    if not articles and verbose:\n",
    "        print('No articles found on Yahoo for', ticker, '- try network/headers or use NewsAPI as a fallback')\n",
    "\n",
    "    return pd.DataFrame(articles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9a2e6c80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fetching https://finance.yahoo.com/quote/AAPL/news?p=AAPL\n",
      "404 for https://finance.yahoo.com/quote/AAPL/news?p=AAPL\n",
      "fetching https://finance.yahoo.com/quote/AAPL/news\n",
      "404 for https://finance.yahoo.com/quote/AAPL/news\n",
      "fetching https://finance.yahoo.com/quote/AAPL\n",
      "404 for https://finance.yahoo.com/quote/AAPL/news?p=AAPL\n",
      "fetching https://finance.yahoo.com/quote/AAPL/news\n",
      "404 for https://finance.yahoo.com/quote/AAPL/news\n",
      "fetching https://finance.yahoo.com/quote/AAPL\n",
      "yahoo items: 36\n",
      "Cells added: NewsAPI fetcher and Yahoo scraper.\n",
      "Use the commented example calls to test locally.\n",
      "yahoo items: 36\n",
      "Cells added: NewsAPI fetcher and Yahoo scraper.\n",
      "Use the commented example calls to test locally.\n"
     ]
    }
   ],
   "source": [
    "# Examples / quick tests (run locally)\n",
    "# 1) NewsAPI (requires API key)\n",
    "# df = fetch_news_newsapi('AAPL OR Apple', '2024-01-01', '2025-10-01', api_key='YOUR_API_KEY', page_size=100)\n",
    "# print('newsapi rows:', len(df))\n",
    "\n",
    "# 2) Yahoo scraper (best-effort)\n",
    "df_y = fetch_news_yahoo('AAPL')\n",
    "print('yahoo items:', len(df_y))\n",
    "df_y.head()\n",
    "\n",
    "print('Cells added: NewsAPI fetcher and Yahoo scraper.\\nUse the commented example calls to test locally.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a73225",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a1ee9a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to load FinBERT using safetensors (use_safetensors=True) ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded FinBERT with safetensors.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "# Robust model loader: prefer safetensors (avoids torch.load vulnerability),\n",
    "# otherwise require torch >= 2.6.0 so transformers can safely call torch.load.\n",
    "def _torch_version_ok(min_ver=\"2.6.0\"):\n",
    "    try:\n",
    "        # packaging is the most reliable; fall back to simple parsing if missing\n",
    "        from packaging.version import parse as _parse\n",
    "        v = getattr(torch, '__version__', '0.0.0').split('+')[0]\n",
    "        return _parse(v) >= _parse(min_ver)\n",
    "    except Exception:\n",
    "        try:\n",
    "            v = getattr(torch, '__version__', '0.0.0').split('+')[0]\n",
    "            major, minor = v.split('.')[:2]\n",
    "            return (int(major), int(minor)) >= tuple(int(x) for x in min_ver.split('.')[:2])\n",
    "        except Exception:\n",
    "            return False\n",
    "\n",
    "\n",
    "def _try_load_finbert():\n",
    "    # First attempt: load safetensors weights (if present on HF) — avoids torch.load\n",
    "    try:\n",
    "        print('Trying to load FinBERT using safetensors (use_safetensors=True) ...')\n",
    "        tokenizer = AutoTokenizer.from_pretrained('ProsusAI/finbert')\n",
    "        model = AutoModelForSequenceClassification.from_pretrained('ProsusAI/finbert', use_safetensors=True)\n",
    "        pipe = pipeline('sentiment-analysis', model=model, tokenizer=tokenizer)\n",
    "        # expose raw model/tokenizer for probability-based scoring\n",
    "        globals()['finbert_model'] = model\n",
    "        globals()['finbert_tokenizer'] = tokenizer\n",
    "        print('Loaded FinBERT with safetensors.')\n",
    "        return pipe\n",
    "    except Exception as e_s:\n",
    "        if hasattr(e_s, '__class__'):\n",
    "            # Keep message brief but informative\n",
    "            print('safetensors load failed or not available:', e_s)\n",
    "        else:\n",
    "            print('safetensors load failed (no further details)')\n",
    "\n",
    "    # If safetensors failed, ensure torch is new enough for transformers to call torch.load safely\n",
    "    if not _torch_version_ok('2.6.0'):\n",
    "        print('Current torch version:', getattr(torch, '__version__', None))\n",
    "        print('Transformer loading requires torch >= 2.6.0 due to a security fix.')\n",
    "        raise RuntimeError('Please upgrade torch to >= 2.6.0 to load FinBERT safely, or ensure safetensors are available for this model.')\n",
    "\n",
    "    # Try loading the standard HF pipeline (this will use torch.load internally)\n",
    "    try:\n",
    "        print('Attempting to load FinBERT pipeline using standard weights...')\n",
    "        pipe = pipeline('sentiment-analysis', model='ProsusAI/finbert', tokenizer='ProsusAI/finbert')\n",
    "        # pipeline contains model/tokenizer attributes we can use for logits\n",
    "        try:\n",
    "            globals()['finbert_model'] = pipe.model\n",
    "            globals()['finbert_tokenizer'] = pipe.tokenizer\n",
    "        except Exception:\n",
    "            # best-effort; if unavailable, finbert_model may remain unset and we'll fallback\n",
    "            pass\n",
    "        print('Loaded FinBERT pipeline (standard weights).')\n",
    "        return pipe\n",
    "    except Exception as e_p:\n",
    "        print('Failed to load FinBERT pipeline with standard weights:', e_p)\n",
    "        raise\n",
    "\n",
    "# Create pipeline object (finbert) or leave None with helpful message\n",
    "try:\n",
    "    finbert = _try_load_finbert()\n",
    "except Exception as e_final:\n",
    "    finbert = None\n",
    "    print('FinBERT is not available in this environment. See messages above to upgrade torch or enable safetensors.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "44eea755",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sentiment_batch(texts, backend='auto'):\n",
    "    \"\"\"Analyze sentiment for a batch of texts.\n",
    "\n",
    "    Parameters\n",
    "    - texts: list[str]\n",
    "    - backend: 'auto'|'finbert'|'nlptown' ;\n",
    "        'auto' will prefer nlptown (intensity-aware 1-5 star) if available, otherwise FinBERT.\n",
    "\n",
    "    Returns\n",
    "    - list[int]: scores in range [-10..10]\n",
    "    \"\"\"\n",
    "    # Helper mappers\n",
    "    def map_nlptown_label_to_score(label):\n",
    "        import re\n",
    "        m = re.search(r'([1-5])', (label or ''))\n",
    "        if not m:\n",
    "            return 0\n",
    "        stars = int(m.group(1))\n",
    "        return int(round(((stars - 3) / 2.0) * 10.0))\n",
    "\n",
    "    def map_prob_to_score(label, prob):\n",
    "        lab = (label or '').lower()\n",
    "        sign = 0\n",
    "        if 'pos' in lab or 'positive' in lab:\n",
    "            sign = 1\n",
    "        elif 'neg' in lab or 'negative' in lab:\n",
    "            sign = -1\n",
    "        else:\n",
    "            return 0\n",
    "        intensity = max(0.0, (float(prob) - 0.5) / 0.5)\n",
    "        return int(round(sign * intensity * 10.0))\n",
    "\n",
    "    # Decide backend\n",
    "    chosen = backend\n",
    "    if backend == 'auto':\n",
    "        # prefer nlptown (1-5 star mapping) since it produced better separation in your diagnostic\n",
    "        chosen = 'nlptown'\n",
    "\n",
    "    # NLPTOWN backend\n",
    "    if chosen == 'nlptown':\n",
    "        try:\n",
    "            nlptown_pipe = globals().get('nlptown_pipe')\n",
    "            if nlptown_pipe is None:\n",
    "                # lazily create and cache\n",
    "                from transformers import pipeline as _pipeline\n",
    "                nlptown_pipe = _pipeline('sentiment-analysis', model='nlptown/bert-base-multilingual-uncased-sentiment', tokenizer='nlptown/bert-base-multilingual-uncased-sentiment')\n",
    "                globals()['nlptown_pipe'] = nlptown_pipe\n",
    "\n",
    "            # Use top_k=None / return_all_scores behavior for distributions when available\n",
    "            try:\n",
    "                dist = nlptown_pipe(texts, return_all_scores=True)\n",
    "            except TypeError:\n",
    "                # older transformers may require single-call or not support return_all_scores\n",
    "                dist = [nlptown_pipe(t, return_all_scores=True) if isinstance(t, str) else [] for t in texts]\n",
    "\n",
    "            mapped = []\n",
    "            for d in dist:\n",
    "                if isinstance(d, list) and len(d) > 0:\n",
    "                    top = sorted(d, key=lambda x: x.get('score', 0.0), reverse=True)[0]\n",
    "                    lab = top.get('label', '')\n",
    "                    mapped.append(map_nlptown_label_to_score(lab))\n",
    "                else:\n",
    "                    mapped.append(0)\n",
    "            return mapped\n",
    "        except Exception as e:\n",
    "            print('nlptown backend failed, falling back to FinBERT logic:', e)\n",
    "            # fall through to finbert logic below\n",
    "\n",
    "    # FINBERT (original logic) - attempt to use the existing finbert model/pipeline\n",
    "    if finbert is None:\n",
    "        raise RuntimeError('FinBERT pipeline is not loaded and nlptown backend failed. Upgrade torch or enable safetensors or run the nlptown diagnostic cell.')\n",
    "\n",
    "    # Keep previous label/margin-based logic for finbert\n",
    "    intensity_kw = {\n",
    "        'very': 1.0,\n",
    "        'strong': 1.0,\n",
    "        'strongly': 1.0,\n",
    "        'high': 0.95,\n",
    "        'heavy': 0.95,\n",
    "        'severe': 0.95,\n",
    "        'major': 0.9,\n",
    "        'sharp': 0.9,\n",
    "        'plunge': 0.95,\n",
    "        'plummets': 1.0,\n",
    "        'surge': 0.95,\n",
    "        'record': 0.9,\n",
    "        'huge': 1.0,\n",
    "        'massive': 1.0,\n",
    "        'slight': 0.35,\n",
    "        'slightly': 0.35,\n",
    "        'modest': 0.45,\n",
    "        'minor': 0.4,\n",
    "        'mild': 0.4,\n",
    "        'nominal': 0.3,\n",
    "        'small': 0.35,\n",
    "        'possible': 0.5,\n",
    "        'likely': 0.7,\n",
    "        'announces layoffs': 0.95,\n",
    "        'layoff': 0.95,\n",
    "        'layoffs': 0.95\n",
    "    }\n",
    "\n",
    "    def _label_contains_intensity(label_str):\n",
    "        s = (label_str or '').lower()\n",
    "        for kw, val in intensity_kw.items():\n",
    "            if kw in s:\n",
    "                return float(val)\n",
    "        return None\n",
    "\n",
    "    def _label_sign(label_str):\n",
    "        s = (label_str or '').lower()\n",
    "        if 'pos' in s or 'positive' in s:\n",
    "            return 1\n",
    "        if 'neg' in s or 'negative' in s:\n",
    "            return -1\n",
    "        return 0\n",
    "\n",
    "    finbert_model_obj = globals().get('finbert_model', None)\n",
    "    finbert_tokenizer_obj = globals().get('finbert_tokenizer', None)\n",
    "\n",
    "    # Try label-space intensity hints first (if model provides them)\n",
    "    label_set = []\n",
    "    if finbert_model_obj is not None:\n",
    "        cfg = getattr(finbert_model_obj, 'config', None)\n",
    "        id2label = getattr(cfg, 'id2label', {}) if cfg is not None else {}\n",
    "        label_set = [str(v).lower() for v in id2label.values()] if id2label else []\n",
    "    else:\n",
    "        try:\n",
    "            probe = finbert(['test'], return_all_scores=True)\n",
    "            if probe and isinstance(probe, list) and len(probe) > 0:\n",
    "                label_set = [d.get('label', '').lower() for d in probe[0]]\n",
    "        except Exception:\n",
    "            label_set = []\n",
    "\n",
    "    detected_intensity_labels = any(_label_contains_intensity(l) is not None for l in label_set)\n",
    "\n",
    "    # If detected intensity labels and raw model available\n",
    "    if detected_intensity_labels and finbert_model_obj is not None and finbert_tokenizer_obj is not None:\n",
    "        inputs = finbert_tokenizer_obj(texts, return_tensors='pt', padding=True, truncation=True)\n",
    "        try:\n",
    "            device = next(finbert_model_obj.parameters()).device\n",
    "        except Exception:\n",
    "            device = None\n",
    "        if device is not None:\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = finbert_model_obj(**inputs)\n",
    "            logits = outputs.logits\n",
    "            probs = torch.nn.functional.softmax(logits, dim=1).cpu().numpy()\n",
    "\n",
    "        cfg = getattr(finbert_model_obj, 'config', None)\n",
    "        id2label = {int(k): v.lower() for k, v in getattr(cfg, 'id2label', {}).items()} if cfg is not None else {}\n",
    "        scores = []\n",
    "        for p in probs:\n",
    "            top_idx = int(p.argmax())\n",
    "            top_label = id2label.get(top_idx, '')\n",
    "            sign = _label_sign(top_label)\n",
    "            intensity = _label_contains_intensity(top_label)\n",
    "            if intensity is None:\n",
    "                top2 = _np.partition(p, -2)[-2:]\n",
    "                margin = float(top2[-1] - top2[0]) if len(top2) >= 2 else float(p[top_idx])\n",
    "                intensity = float(_np.tanh(margin / 1.0))\n",
    "            val = int(round(sign * intensity * 10.0))\n",
    "            scores.append(max(-10, min(10, val)))\n",
    "        return scores\n",
    "\n",
    "    # Fallback: margin-based using raw logits if available\n",
    "    if finbert_model_obj is not None and finbert_tokenizer_obj is not None:\n",
    "        inputs = finbert_tokenizer_obj(texts, return_tensors='pt', padding=True, truncation=True)\n",
    "        try:\n",
    "            device = next(finbert_model_obj.parameters()).device\n",
    "        except Exception:\n",
    "            device = None\n",
    "        if device is not None:\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = finbert_model_obj(**inputs)\n",
    "            logits = outputs.logits\n",
    "        top_vals, top_idx = torch.topk(logits, k=2, dim=1)\n",
    "        margins = (top_vals[:, 0] - top_vals[:, 1]).cpu().numpy()\n",
    "        cfg = getattr(finbert_model_obj, 'config', None)\n",
    "        id2label = {int(k): v.lower() for k, v in getattr(cfg, 'id2label', {}).items()} if cfg is not None else {}\n",
    "        signs = []\n",
    "        for idx in top_idx[:, 0].cpu().numpy():\n",
    "            lbl = id2label.get(int(idx), '')\n",
    "            signs.append(_label_sign(lbl))\n",
    "        norm = _np.tanh(margins / 1.0)\n",
    "        scores = [int(round(float(sgn) * float(n) * 10.0)) for sgn, n in zip(signs, norm)]\n",
    "        scores = [max(-10, min(10, v)) for v in scores]\n",
    "        return scores\n",
    "\n",
    "    # Last fallback: pipeline single-shot\n",
    "    results = finbert(texts)\n",
    "    scores = []\n",
    "    for r in results:\n",
    "        label = (r.get('label', '') or '').lower()\n",
    "        sign = _label_sign(label)\n",
    "        conf = float(r.get('score', 0.0) or 0.0)\n",
    "        val = int(round(sign * _np.tanh((conf - 0.5) * 2.0) * 10.0))\n",
    "        scores.append(max(-10, min(10, val)))\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f5b2592c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FinBERT loaded: True\n"
     ]
    }
   ],
   "source": [
    "# FinBERT loader completed in previous cell. Check whether it loaded successfully.\n",
    "print('FinBERT loaded:', finbert is not None)\n",
    "if finbert is None:\n",
    "    print('FinBERT pipeline is not available. Upgrade torch to >=2.6 or enable safetensors as instructed in the previous cell.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "063016a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finbert available: True\n",
      "Sentiment: negative | Score:    -5 | Text: Company posts record a slight loss in profits.\n",
      "Sentiment: negative | Score:   -10 | Text: Company reports heavy losses and announces layoffs.\n",
      "Sentiment:  neutral | Score:     5 | Text: Company schedules conference call to discuss quarterly results.\n",
      "\n",
      "Model: nlptown/bert-base-multilingual-uncased-sentiment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  mapped scores: [-5, -10, 5]\n",
      "\n",
      "Model: cardiffnlp/twitter-roberta-base-sentiment\n",
      "  Could not load model: Could not load model cardiffnlp/twitter-roberta-base-sentiment with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForSequenceClassification'>, <class 'transformers.models.roberta.modeling_roberta.RobertaForSequenceClassification'>). See the original errors:\n",
      "\n",
      "while loading with AutoModelForSequenceClassification, an error is thrown:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/pham156/.conda/envs/rocky9/2024.09/CS373/lib/python3.11/site-packages/transformers/pipelines/base.py\", line 293, in infer_framework_load_model\n",
      "    model = model_class.from_pretrained(model, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/pham156/.conda/envs/rocky9/2024.09/CS373/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py\", line 604, in from_pretrained\n",
      "    return model_class.from_pretrained(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/pham156/.conda/envs/rocky9/2024.09/CS373/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 277, in _wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/pham156/.conda/envs/rocky9/2024.09/CS373/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 4962, in from_pretrained\n",
      "    config, dtype, dtype_orig = _get_dtype(\n",
      "                                ^^^^^^^^^^^\n",
      "  File \"/home/pham156/.conda/envs/rocky9/2024.09/CS373/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 1250, in _get_dtype\n",
      "    state_dict = load_state_dict(\n",
      "                 ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/pham156/.conda/envs/rocky9/2024.09/CS373/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 508, in load_state_dict\n",
      "    check_torch_load_is_safe()\n",
      "  File \"/home/pham156/.conda/envs/rocky9/2024.09/CS373/lib/python3.11/site-packages/transformers/utils/import_utils.py\", line 1647, in check_torch_load_is_safe\n",
      "    raise ValueError(\n",
      "ValueError: Due to a serious vulnerability issue in `torch.load`, even with `weights_only=True`, we now require users to upgrade torch to at least v2.6 in order to use the function. This version restriction does not apply when loading files with safetensors.\n",
      "See the vulnerability report here https://nvd.nist.gov/vuln/detail/CVE-2025-32434\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/pham156/.conda/envs/rocky9/2024.09/CS373/lib/python3.11/site-packages/transformers/pipelines/base.py\", line 311, in infer_framework_load_model\n",
      "    model = model_class.from_pretrained(model, **fp32_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/pham156/.conda/envs/rocky9/2024.09/CS373/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py\", line 604, in from_pretrained\n",
      "    return model_class.from_pretrained(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/pham156/.conda/envs/rocky9/2024.09/CS373/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 277, in _wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/pham156/.conda/envs/rocky9/2024.09/CS373/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 5048, in from_pretrained\n",
      "    ) = cls._load_pretrained_model(\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/pham156/.conda/envs/rocky9/2024.09/CS373/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 5316, in _load_pretrained_model\n",
      "    load_state_dict(checkpoint_files[0], map_location=\"meta\", weights_only=weights_only).keys()\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/pham156/.conda/envs/rocky9/2024.09/CS373/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 508, in load_state_dict\n",
      "    check_torch_load_is_safe()\n",
      "  File \"/home/pham156/.conda/envs/rocky9/2024.09/CS373/lib/python3.11/site-packages/transformers/utils/import_utils.py\", line 1647, in check_torch_load_is_safe\n",
      "    raise ValueError(\n",
      "ValueError: Due to a serious vulnerability issue in `torch.load`, even with `weights_only=True`, we now require users to upgrade torch to at least v2.6 in order to use the function. This version restriction does not apply when loading files with safetensors.\n",
      "See the vulnerability report here https://nvd.nist.gov/vuln/detail/CVE-2025-32434\n",
      "\n",
      "while loading with RobertaForSequenceClassification, an error is thrown:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/pham156/.conda/envs/rocky9/2024.09/CS373/lib/python3.11/site-packages/transformers/pipelines/base.py\", line 293, in infer_framework_load_model\n",
      "    model = model_class.from_pretrained(model, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/pham156/.conda/envs/rocky9/2024.09/CS373/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 277, in _wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/pham156/.conda/envs/rocky9/2024.09/CS373/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 4962, in from_pretrained\n",
      "    config, dtype, dtype_orig = _get_dtype(\n",
      "                                ^^^^^^^^^^^\n",
      "  File \"/home/pham156/.conda/envs/rocky9/2024.09/CS373/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 1250, in _get_dtype\n",
      "    state_dict = load_state_dict(\n",
      "                 ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/pham156/.conda/envs/rocky9/2024.09/CS373/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 508, in load_state_dict\n",
      "    check_torch_load_is_safe()\n",
      "  File \"/home/pham156/.conda/envs/rocky9/2024.09/CS373/lib/python3.11/site-packages/transformers/utils/import_utils.py\", line 1647, in check_torch_load_is_safe\n",
      "    raise ValueError(\n",
      "ValueError: Due to a serious vulnerability issue in `torch.load`, even with `weights_only=True`, we now require users to upgrade torch to at least v2.6 in order to use the function. This version restriction does not apply when loading files with safetensors.\n",
      "See the vulnerability report here https://nvd.nist.gov/vuln/detail/CVE-2025-32434\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/pham156/.conda/envs/rocky9/2024.09/CS373/lib/python3.11/site-packages/transformers/pipelines/base.py\", line 311, in infer_framework_load_model\n",
      "    model = model_class.from_pretrained(model, **fp32_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/pham156/.conda/envs/rocky9/2024.09/CS373/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 277, in _wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/pham156/.conda/envs/rocky9/2024.09/CS373/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 5048, in from_pretrained\n",
      "    ) = cls._load_pretrained_model(\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/pham156/.conda/envs/rocky9/2024.09/CS373/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 5316, in _load_pretrained_model\n",
      "    load_state_dict(checkpoint_files[0], map_location=\"meta\", weights_only=weights_only).keys()\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/pham156/.conda/envs/rocky9/2024.09/CS373/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 508, in load_state_dict\n",
      "    check_torch_load_is_safe()\n",
      "  File \"/home/pham156/.conda/envs/rocky9/2024.09/CS373/lib/python3.11/site-packages/transformers/utils/import_utils.py\", line 1647, in check_torch_load_is_safe\n",
      "    raise ValueError(\n",
      "ValueError: Due to a serious vulnerability issue in `torch.load`, even with `weights_only=True`, we now require users to upgrade torch to at least v2.6 in order to use the function. This version restriction does not apply when loading files with safetensors.\n",
      "See the vulnerability report here https://nvd.nist.gov/vuln/detail/CVE-2025-32434\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Model: finiteautomata/bertweet-base-sentiment-analysis\n",
      "  Could not load model: Could not load model cardiffnlp/twitter-roberta-base-sentiment with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForSequenceClassification'>, <class 'transformers.models.roberta.modeling_roberta.RobertaForSequenceClassification'>). See the original errors:\n",
      "\n",
      "while loading with AutoModelForSequenceClassification, an error is thrown:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/pham156/.conda/envs/rocky9/2024.09/CS373/lib/python3.11/site-packages/transformers/pipelines/base.py\", line 293, in infer_framework_load_model\n",
      "    model = model_class.from_pretrained(model, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/pham156/.conda/envs/rocky9/2024.09/CS373/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py\", line 604, in from_pretrained\n",
      "    return model_class.from_pretrained(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/pham156/.conda/envs/rocky9/2024.09/CS373/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 277, in _wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/pham156/.conda/envs/rocky9/2024.09/CS373/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 4962, in from_pretrained\n",
      "    config, dtype, dtype_orig = _get_dtype(\n",
      "                                ^^^^^^^^^^^\n",
      "  File \"/home/pham156/.conda/envs/rocky9/2024.09/CS373/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 1250, in _get_dtype\n",
      "    state_dict = load_state_dict(\n",
      "                 ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/pham156/.conda/envs/rocky9/2024.09/CS373/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 508, in load_state_dict\n",
      "    check_torch_load_is_safe()\n",
      "  File \"/home/pham156/.conda/envs/rocky9/2024.09/CS373/lib/python3.11/site-packages/transformers/utils/import_utils.py\", line 1647, in check_torch_load_is_safe\n",
      "    raise ValueError(\n",
      "ValueError: Due to a serious vulnerability issue in `torch.load`, even with `weights_only=True`, we now require users to upgrade torch to at least v2.6 in order to use the function. This version restriction does not apply when loading files with safetensors.\n",
      "See the vulnerability report here https://nvd.nist.gov/vuln/detail/CVE-2025-32434\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/pham156/.conda/envs/rocky9/2024.09/CS373/lib/python3.11/site-packages/transformers/pipelines/base.py\", line 311, in infer_framework_load_model\n",
      "    model = model_class.from_pretrained(model, **fp32_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/pham156/.conda/envs/rocky9/2024.09/CS373/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py\", line 604, in from_pretrained\n",
      "    return model_class.from_pretrained(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/pham156/.conda/envs/rocky9/2024.09/CS373/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 277, in _wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/pham156/.conda/envs/rocky9/2024.09/CS373/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 5048, in from_pretrained\n",
      "    ) = cls._load_pretrained_model(\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/pham156/.conda/envs/rocky9/2024.09/CS373/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 5316, in _load_pretrained_model\n",
      "    load_state_dict(checkpoint_files[0], map_location=\"meta\", weights_only=weights_only).keys()\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/pham156/.conda/envs/rocky9/2024.09/CS373/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 508, in load_state_dict\n",
      "    check_torch_load_is_safe()\n",
      "  File \"/home/pham156/.conda/envs/rocky9/2024.09/CS373/lib/python3.11/site-packages/transformers/utils/import_utils.py\", line 1647, in check_torch_load_is_safe\n",
      "    raise ValueError(\n",
      "ValueError: Due to a serious vulnerability issue in `torch.load`, even with `weights_only=True`, we now require users to upgrade torch to at least v2.6 in order to use the function. This version restriction does not apply when loading files with safetensors.\n",
      "See the vulnerability report here https://nvd.nist.gov/vuln/detail/CVE-2025-32434\n",
      "\n",
      "while loading with RobertaForSequenceClassification, an error is thrown:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/pham156/.conda/envs/rocky9/2024.09/CS373/lib/python3.11/site-packages/transformers/pipelines/base.py\", line 293, in infer_framework_load_model\n",
      "    model = model_class.from_pretrained(model, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/pham156/.conda/envs/rocky9/2024.09/CS373/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 277, in _wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/pham156/.conda/envs/rocky9/2024.09/CS373/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 4962, in from_pretrained\n",
      "    config, dtype, dtype_orig = _get_dtype(\n",
      "                                ^^^^^^^^^^^\n",
      "  File \"/home/pham156/.conda/envs/rocky9/2024.09/CS373/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 1250, in _get_dtype\n",
      "    state_dict = load_state_dict(\n",
      "                 ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/pham156/.conda/envs/rocky9/2024.09/CS373/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 508, in load_state_dict\n",
      "    check_torch_load_is_safe()\n",
      "  File \"/home/pham156/.conda/envs/rocky9/2024.09/CS373/lib/python3.11/site-packages/transformers/utils/import_utils.py\", line 1647, in check_torch_load_is_safe\n",
      "    raise ValueError(\n",
      "ValueError: Due to a serious vulnerability issue in `torch.load`, even with `weights_only=True`, we now require users to upgrade torch to at least v2.6 in order to use the function. This version restriction does not apply when loading files with safetensors.\n",
      "See the vulnerability report here https://nvd.nist.gov/vuln/detail/CVE-2025-32434\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/pham156/.conda/envs/rocky9/2024.09/CS373/lib/python3.11/site-packages/transformers/pipelines/base.py\", line 311, in infer_framework_load_model\n",
      "    model = model_class.from_pretrained(model, **fp32_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/pham156/.conda/envs/rocky9/2024.09/CS373/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 277, in _wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/pham156/.conda/envs/rocky9/2024.09/CS373/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 5048, in from_pretrained\n",
      "    ) = cls._load_pretrained_model(\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/pham156/.conda/envs/rocky9/2024.09/CS373/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 5316, in _load_pretrained_model\n",
      "    load_state_dict(checkpoint_files[0], map_location=\"meta\", weights_only=weights_only).keys()\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/pham156/.conda/envs/rocky9/2024.09/CS373/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 508, in load_state_dict\n",
      "    check_torch_load_is_safe()\n",
      "  File \"/home/pham156/.conda/envs/rocky9/2024.09/CS373/lib/python3.11/site-packages/transformers/utils/import_utils.py\", line 1647, in check_torch_load_is_safe\n",
      "    raise ValueError(\n",
      "ValueError: Due to a serious vulnerability issue in `torch.load`, even with `weights_only=True`, we now require users to upgrade torch to at least v2.6 in order to use the function. This version restriction does not apply when loading files with safetensors.\n",
      "See the vulnerability report here https://nvd.nist.gov/vuln/detail/CVE-2025-32434\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Model: finiteautomata/bertweet-base-sentiment-analysis\n",
      "  Could not load model: Could not load model finiteautomata/bertweet-base-sentiment-analysis with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForSequenceClassification'>, <class 'transformers.models.roberta.modeling_roberta.RobertaForSequenceClassification'>). See the original errors:\n",
      "\n",
      "while loading with AutoModelForSequenceClassification, an error is thrown:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/pham156/.conda/envs/rocky9/2024.09/CS373/lib/python3.11/site-packages/transformers/pipelines/base.py\", line 293, in infer_framework_load_model\n",
      "    model = model_class.from_pretrained(model, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/pham156/.conda/envs/rocky9/2024.09/CS373/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py\", line 604, in from_pretrained\n",
      "    return model_class.from_pretrained(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/pham156/.conda/envs/rocky9/2024.09/CS373/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 277, in _wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/pham156/.conda/envs/rocky9/2024.09/CS373/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 5048, in from_pretrained\n",
      "    ) = cls._load_pretrained_model(\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/pham156/.conda/envs/rocky9/2024.09/CS373/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 5316, in _load_pretrained_model\n",
      "    load_state_dict(checkpoint_files[0], map_location=\"meta\", weights_only=weights_only).keys()\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/pham156/.conda/envs/rocky9/2024.09/CS373/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 508, in load_state_dict\n",
      "    check_torch_load_is_safe()\n",
      "  File \"/home/pham156/.conda/envs/rocky9/2024.09/CS373/lib/python3.11/site-packages/transformers/utils/import_utils.py\", line 1647, in check_torch_load_is_safe\n",
      "    raise ValueError(\n",
      "ValueError: Due to a serious vulnerability issue in `torch.load`, even with `weights_only=True`, we now require users to upgrade torch to at least v2.6 in order to use the function. This version restriction does not apply when loading files with safetensors.\n",
      "See the vulnerability report here https://nvd.nist.gov/vuln/detail/CVE-2025-32434\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/pham156/.conda/envs/rocky9/2024.09/CS373/lib/python3.11/site-packages/transformers/pipelines/base.py\", line 311, in infer_framework_load_model\n",
      "    model = model_class.from_pretrained(model, **fp32_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/pham156/.conda/envs/rocky9/2024.09/CS373/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py\", line 604, in from_pretrained\n",
      "    return model_class.from_pretrained(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/pham156/.conda/envs/rocky9/2024.09/CS373/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 277, in _wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/pham156/.conda/envs/rocky9/2024.09/CS373/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 5048, in from_pretrained\n",
      "    ) = cls._load_pretrained_model(\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/pham156/.conda/envs/rocky9/2024.09/CS373/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 5316, in _load_pretrained_model\n",
      "    load_state_dict(checkpoint_files[0], map_location=\"meta\", weights_only=weights_only).keys()\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/pham156/.conda/envs/rocky9/2024.09/CS373/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 508, in load_state_dict\n",
      "    check_torch_load_is_safe()\n",
      "  File \"/home/pham156/.conda/envs/rocky9/2024.09/CS373/lib/python3.11/site-packages/transformers/utils/import_utils.py\", line 1647, in check_torch_load_is_safe\n",
      "    raise ValueError(\n",
      "ValueError: Due to a serious vulnerability issue in `torch.load`, even with `weights_only=True`, we now require users to upgrade torch to at least v2.6 in order to use the function. This version restriction does not apply when loading files with safetensors.\n",
      "See the vulnerability report here https://nvd.nist.gov/vuln/detail/CVE-2025-32434\n",
      "\n",
      "while loading with RobertaForSequenceClassification, an error is thrown:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/pham156/.conda/envs/rocky9/2024.09/CS373/lib/python3.11/site-packages/transformers/pipelines/base.py\", line 293, in infer_framework_load_model\n",
      "    model = model_class.from_pretrained(model, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/pham156/.conda/envs/rocky9/2024.09/CS373/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 277, in _wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/pham156/.conda/envs/rocky9/2024.09/CS373/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 5048, in from_pretrained\n",
      "    ) = cls._load_pretrained_model(\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/pham156/.conda/envs/rocky9/2024.09/CS373/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 5316, in _load_pretrained_model\n",
      "    load_state_dict(checkpoint_files[0], map_location=\"meta\", weights_only=weights_only).keys()\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/pham156/.conda/envs/rocky9/2024.09/CS373/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 508, in load_state_dict\n",
      "    check_torch_load_is_safe()\n",
      "  File \"/home/pham156/.conda/envs/rocky9/2024.09/CS373/lib/python3.11/site-packages/transformers/utils/import_utils.py\", line 1647, in check_torch_load_is_safe\n",
      "    raise ValueError(\n",
      "ValueError: Due to a serious vulnerability issue in `torch.load`, even with `weights_only=True`, we now require users to upgrade torch to at least v2.6 in order to use the function. This version restriction does not apply when loading files with safetensors.\n",
      "See the vulnerability report here https://nvd.nist.gov/vuln/detail/CVE-2025-32434\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/pham156/.conda/envs/rocky9/2024.09/CS373/lib/python3.11/site-packages/transformers/pipelines/base.py\", line 311, in infer_framework_load_model\n",
      "    model = model_class.from_pretrained(model, **fp32_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/pham156/.conda/envs/rocky9/2024.09/CS373/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 277, in _wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/pham156/.conda/envs/rocky9/2024.09/CS373/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 5048, in from_pretrained\n",
      "    ) = cls._load_pretrained_model(\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/pham156/.conda/envs/rocky9/2024.09/CS373/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 5316, in _load_pretrained_model\n",
      "    load_state_dict(checkpoint_files[0], map_location=\"meta\", weights_only=weights_only).keys()\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/pham156/.conda/envs/rocky9/2024.09/CS373/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 508, in load_state_dict\n",
      "    check_torch_load_is_safe()\n",
      "  File \"/home/pham156/.conda/envs/rocky9/2024.09/CS373/lib/python3.11/site-packages/transformers/utils/import_utils.py\", line 1647, in check_torch_load_is_safe\n",
      "    raise ValueError(\n",
      "ValueError: Due to a serious vulnerability issue in `torch.load`, even with `weights_only=True`, we now require users to upgrade torch to at least v2.6 in order to use the function. This version restriction does not apply when loading files with safetensors.\n",
      "See the vulnerability report here https://nvd.nist.gov/vuln/detail/CVE-2025-32434\n",
      "\n",
      "\n",
      "\n",
      "  Could not load model: Could not load model finiteautomata/bertweet-base-sentiment-analysis with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForSequenceClassification'>, <class 'transformers.models.roberta.modeling_roberta.RobertaForSequenceClassification'>). See the original errors:\n",
      "\n",
      "while loading with AutoModelForSequenceClassification, an error is thrown:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/pham156/.conda/envs/rocky9/2024.09/CS373/lib/python3.11/site-packages/transformers/pipelines/base.py\", line 293, in infer_framework_load_model\n",
      "    model = model_class.from_pretrained(model, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/pham156/.conda/envs/rocky9/2024.09/CS373/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py\", line 604, in from_pretrained\n",
      "    return model_class.from_pretrained(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/pham156/.conda/envs/rocky9/2024.09/CS373/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 277, in _wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/pham156/.conda/envs/rocky9/2024.09/CS373/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 5048, in from_pretrained\n",
      "    ) = cls._load_pretrained_model(\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/pham156/.conda/envs/rocky9/2024.09/CS373/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 5316, in _load_pretrained_model\n",
      "    load_state_dict(checkpoint_files[0], map_location=\"meta\", weights_only=weights_only).keys()\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/pham156/.conda/envs/rocky9/2024.09/CS373/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 508, in load_state_dict\n",
      "    check_torch_load_is_safe()\n",
      "  File \"/home/pham156/.conda/envs/rocky9/2024.09/CS373/lib/python3.11/site-packages/transformers/utils/import_utils.py\", line 1647, in check_torch_load_is_safe\n",
      "    raise ValueError(\n",
      "ValueError: Due to a serious vulnerability issue in `torch.load`, even with `weights_only=True`, we now require users to upgrade torch to at least v2.6 in order to use the function. This version restriction does not apply when loading files with safetensors.\n",
      "See the vulnerability report here https://nvd.nist.gov/vuln/detail/CVE-2025-32434\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/pham156/.conda/envs/rocky9/2024.09/CS373/lib/python3.11/site-packages/transformers/pipelines/base.py\", line 311, in infer_framework_load_model\n",
      "    model = model_class.from_pretrained(model, **fp32_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/pham156/.conda/envs/rocky9/2024.09/CS373/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py\", line 604, in from_pretrained\n",
      "    return model_class.from_pretrained(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/pham156/.conda/envs/rocky9/2024.09/CS373/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 277, in _wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/pham156/.conda/envs/rocky9/2024.09/CS373/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 5048, in from_pretrained\n",
      "    ) = cls._load_pretrained_model(\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/pham156/.conda/envs/rocky9/2024.09/CS373/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 5316, in _load_pretrained_model\n",
      "    load_state_dict(checkpoint_files[0], map_location=\"meta\", weights_only=weights_only).keys()\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/pham156/.conda/envs/rocky9/2024.09/CS373/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 508, in load_state_dict\n",
      "    check_torch_load_is_safe()\n",
      "  File \"/home/pham156/.conda/envs/rocky9/2024.09/CS373/lib/python3.11/site-packages/transformers/utils/import_utils.py\", line 1647, in check_torch_load_is_safe\n",
      "    raise ValueError(\n",
      "ValueError: Due to a serious vulnerability issue in `torch.load`, even with `weights_only=True`, we now require users to upgrade torch to at least v2.6 in order to use the function. This version restriction does not apply when loading files with safetensors.\n",
      "See the vulnerability report here https://nvd.nist.gov/vuln/detail/CVE-2025-32434\n",
      "\n",
      "while loading with RobertaForSequenceClassification, an error is thrown:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/pham156/.conda/envs/rocky9/2024.09/CS373/lib/python3.11/site-packages/transformers/pipelines/base.py\", line 293, in infer_framework_load_model\n",
      "    model = model_class.from_pretrained(model, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/pham156/.conda/envs/rocky9/2024.09/CS373/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 277, in _wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/pham156/.conda/envs/rocky9/2024.09/CS373/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 5048, in from_pretrained\n",
      "    ) = cls._load_pretrained_model(\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/pham156/.conda/envs/rocky9/2024.09/CS373/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 5316, in _load_pretrained_model\n",
      "    load_state_dict(checkpoint_files[0], map_location=\"meta\", weights_only=weights_only).keys()\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/pham156/.conda/envs/rocky9/2024.09/CS373/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 508, in load_state_dict\n",
      "    check_torch_load_is_safe()\n",
      "  File \"/home/pham156/.conda/envs/rocky9/2024.09/CS373/lib/python3.11/site-packages/transformers/utils/import_utils.py\", line 1647, in check_torch_load_is_safe\n",
      "    raise ValueError(\n",
      "ValueError: Due to a serious vulnerability issue in `torch.load`, even with `weights_only=True`, we now require users to upgrade torch to at least v2.6 in order to use the function. This version restriction does not apply when loading files with safetensors.\n",
      "See the vulnerability report here https://nvd.nist.gov/vuln/detail/CVE-2025-32434\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/pham156/.conda/envs/rocky9/2024.09/CS373/lib/python3.11/site-packages/transformers/pipelines/base.py\", line 311, in infer_framework_load_model\n",
      "    model = model_class.from_pretrained(model, **fp32_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/pham156/.conda/envs/rocky9/2024.09/CS373/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 277, in _wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/pham156/.conda/envs/rocky9/2024.09/CS373/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 5048, in from_pretrained\n",
      "    ) = cls._load_pretrained_model(\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/pham156/.conda/envs/rocky9/2024.09/CS373/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 5316, in _load_pretrained_model\n",
      "    load_state_dict(checkpoint_files[0], map_location=\"meta\", weights_only=weights_only).keys()\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/pham156/.conda/envs/rocky9/2024.09/CS373/lib/python3.11/site-packages/transformers/modeling_utils.py\", line 508, in load_state_dict\n",
      "    check_torch_load_is_safe()\n",
      "  File \"/home/pham156/.conda/envs/rocky9/2024.09/CS373/lib/python3.11/site-packages/transformers/utils/import_utils.py\", line 1647, in check_torch_load_is_safe\n",
      "    raise ValueError(\n",
      "ValueError: Due to a serious vulnerability issue in `torch.load`, even with `weights_only=True`, we now require users to upgrade torch to at least v2.6 in order to use the function. This version restriction does not apply when loading files with safetensors.\n",
      "See the vulnerability report here https://nvd.nist.gov/vuln/detail/CVE-2025-32434\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Very simple examples: one positive, one negative, one neutral headline\n",
    "print('finbert available:', finbert is not None)\n",
    "\n",
    "examples = [\n",
    "    ('negative', 'Company posts record a slight loss in profits.'),\n",
    "    ('negative', 'Company reports heavy losses and announces layoffs.'),\n",
    "    ('neutral', 'Company schedules conference call to discuss quarterly results.')\n",
    "]\n",
    "\n",
    "if finbert is None:\n",
    "    print('FinBERT pipeline not loaded — see previous cell for instructions to upgrade torch or enable safetensors.')\n",
    "else:\n",
    "    texts = [t for _, t in examples]\n",
    "    scores = analyze_sentiment_batch(texts)\n",
    "    for (label, text), score in zip(examples, scores):\n",
    "        print(f\"Sentiment: {label:>8} | Score: {score:>5} | Text: {text}\")\n",
    "\n",
    "# End of simple examples\n",
    "\n",
    "# --- Diagnostic: try alternative open-source sentiment models and map to -10..10 ---\n",
    "# This cell attempts several publicly-available models to see whether any give a more\n",
    "# intensity-sensitive output for the example headlines. Run this cell locally (it will\n",
    "# download models from Hugging Face) and inspect printed scores to decide which to use.\n",
    "\n",
    "from transformers import pipeline\n",
    "import numpy as _np\n",
    "\n",
    "candidate_models = [\n",
    "    'nlptown/bert-base-multilingual-uncased-sentiment',\n",
    "    'cardiffnlp/twitter-roberta-base-sentiment',\n",
    "    'finiteautomata/bertweet-base-sentiment-analysis',\n",
    "]\n",
    "\n",
    "texts = [t for _, t in examples]\n",
    "\n",
    "\n",
    "def map_nlptown_label_to_score(label):\n",
    "    import re\n",
    "    m = re.search(r'([1-5])', (label or ''))\n",
    "    if not m:\n",
    "        return 0\n",
    "    stars = int(m.group(1))\n",
    "    return int(round(((stars - 3) / 2.0) * 10.0))\n",
    "\n",
    "\n",
    "def map_prob_to_score(label, prob):\n",
    "    lab = (label or '').lower()\n",
    "    sign = 0\n",
    "    if 'pos' in lab or 'positive' in lab:\n",
    "        sign = 1\n",
    "    elif 'neg' in lab or 'negative' in lab:\n",
    "        sign = -1\n",
    "    else:\n",
    "        return 0\n",
    "    intensity = max(0.0, (float(prob) - 0.5) / 0.5)\n",
    "    return int(round(sign * intensity * 10.0))\n",
    "\n",
    "for mid in candidate_models:\n",
    "    print('\\nModel:', mid)\n",
    "    try:\n",
    "        pipe = pipeline('sentiment-analysis', model=mid, tokenizer=mid)\n",
    "    except Exception as e:\n",
    "        print('  Could not load model:', e)\n",
    "        continue\n",
    "\n",
    "    # Try return_all_scores first\n",
    "    try:\n",
    "        dist = pipe(texts, return_all_scores=True)\n",
    "        mapped = []\n",
    "        for d in dist:\n",
    "            if isinstance(d, list) and len(d) > 0:\n",
    "                top = sorted(d, key=lambda x: x.get('score', 0.0), reverse=True)[0]\n",
    "                lab = top.get('label', '')\n",
    "                scr = top.get('score', 0.0)\n",
    "                if 'nlptown' in mid:\n",
    "                    mapped.append(map_nlptown_label_to_score(lab))\n",
    "                else:\n",
    "                    mapped.append(map_prob_to_score(lab, scr))\n",
    "            else:\n",
    "                mapped.append(0)\n",
    "        print('  mapped scores:', mapped)\n",
    "        continue\n",
    "    except TypeError:\n",
    "        # some pipeline variants don't accept return_all_scores for multi-inputs; fall back\n",
    "        pass\n",
    "    except Exception as e:\n",
    "        print('  return_all_scores not available or failed:', e)\n",
    "\n",
    "    try:\n",
    "        res = pipe(texts)\n",
    "        mapped = []\n",
    "        for r in res:\n",
    "            lab = (r.get('label', '') or '').lower()\n",
    "            scr = float(r.get('score', 0.0) or 0.0)\n",
    "            if 'nlptown' in mid:\n",
    "                mapped.append(map_nlptown_label_to_score(r.get('label', '')))\n",
    "            else:\n",
    "                mapped.append(map_prob_to_score(r.get('label', ''), scr))\n",
    "        print('  mapped scores:', mapped)\n",
    "    except Exception as e:\n",
    "        print('  single-shot pipeline call failed:', e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "907bd5b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- NLPTOWN (distribution and mapped score) ---\n",
      "\n",
      "Text: Company posts record a slight loss in profits.\n",
      "   1 star: 0.2466\n",
      "   2 stars: 0.3074\n",
      "   3 stars: 0.3072\n",
      "   4 stars: 0.1093\n",
      "   5 stars: 0.0295\n",
      "  MAPPED (nlptown->-10..10): -5\n",
      "\n",
      "Text: Company reports heavy losses and announces layoffs.\n",
      "   1 star: 0.7153\n",
      "   2 stars: 0.1938\n",
      "   3 stars: 0.0627\n",
      "   4 stars: 0.0168\n",
      "   5 stars: 0.0115\n",
      "  MAPPED (nlptown->-10..10): -10\n",
      "\n",
      "Text: Company schedules conference call to discuss quarterly results.\n",
      "   1 star: 0.0303\n",
      "   2 stars: 0.0480\n",
      "   3 stars: 0.2041\n",
      "   4 stars: 0.4360\n",
      "   5 stars: 0.2816\n",
      "  MAPPED (nlptown->-10..10): 5\n",
      "\n",
      "--- FINBERT (pipeline/probs or raw logits if available) ---\n",
      "\n",
      "Text: Company posts record a slight loss in profits.\n",
      "   positive: 0.0117\n",
      "   negative: 0.9723\n",
      "   neutral: 0.0160\n",
      "  Margin: 0.9562507271766663\n",
      "\n",
      "Text: Company reports heavy losses and announces layoffs.\n",
      "   positive: 0.0077\n",
      "   negative: 0.9687\n",
      "   neutral: 0.0235\n",
      "  Margin: 0.945213794708252\n",
      "\n",
      "Text: Company schedules conference call to discuss quarterly results.\n",
      "   positive: 0.0299\n",
      "   negative: 0.1693\n",
      "   neutral: 0.8008\n",
      "  Margin: 0.6314281225204468\n"
     ]
    }
   ],
   "source": [
    "# Diagnostic: print raw model outputs (probs/logits) for nlptown and FinBERT\n",
    "# Run this cell to see exactly what each model returns for the example texts.\n",
    "\n",
    "texts = [\n",
    "    'Company posts record a slight loss in profits.',\n",
    "    'Company reports heavy losses and announces layoffs.',\n",
    "    'Company schedules conference call to discuss quarterly results.'\n",
    "]\n",
    "\n",
    "print('\\n--- NLPTOWN (distribution and mapped score) ---')\n",
    "try:\n",
    "    nlptown_pipe = globals().get('nlptown_pipe')\n",
    "    if nlptown_pipe is None:\n",
    "        from transformers import pipeline as _pipeline\n",
    "        nlptown_pipe = _pipeline('sentiment-analysis', model='nlptown/bert-base-multilingual-uncased-sentiment', tokenizer='nlptown/bert-base-multilingual-uncased-sentiment')\n",
    "        globals()['nlptown_pipe'] = nlptown_pipe\n",
    "\n",
    "    try:\n",
    "        dist = nlptown_pipe(texts, return_all_scores=True)\n",
    "    except TypeError:\n",
    "        dist = [nlptown_pipe(t, return_all_scores=True) for t in texts]\n",
    "\n",
    "    for t, d in zip(texts, dist):\n",
    "        print('\\nText:', t)\n",
    "        if not isinstance(d, list):\n",
    "            print('  Unexpected dist format:', d)\n",
    "            continue\n",
    "        for entry in sorted(d, key=lambda x: x.get('label','')):\n",
    "            print(f\"   {entry.get('label')}: {entry.get('score'):.4f}\")\n",
    "        # mapped score\n",
    "        top = sorted(d, key=lambda x: x.get('score', 0.0), reverse=True)[0]\n",
    "        import re\n",
    "        m = re.search(r'([1-5])', (top.get('label','') or ''))\n",
    "        mapped = int(round(((int(m.group(1)) - 3) / 2.0) * 10.0)) if m else 0\n",
    "        print('  MAPPED (nlptown->-10..10):', mapped)\n",
    "\n",
    "except Exception as e:\n",
    "    print('NLPTOWN diagnostic failed:', e)\n",
    "\n",
    "print('\\n--- FINBERT (pipeline/probs or raw logits if available) ---')\n",
    "try:\n",
    "    finbert_model_obj = globals().get('finbert_model', None)\n",
    "    finbert_tokenizer_obj = globals().get('finbert_tokenizer', None)\n",
    "    if finbert_model_obj is not None and finbert_tokenizer_obj is not None:\n",
    "        # raw logits route\n",
    "        inputs = finbert_tokenizer_obj(texts, return_tensors='pt', padding=True, truncation=True)\n",
    "        try:\n",
    "            device = next(finbert_model_obj.parameters()).device\n",
    "        except Exception:\n",
    "            device = None\n",
    "        if device is not None:\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        import torch as _torch\n",
    "        with _torch.no_grad():\n",
    "            outputs = finbert_model_obj(**inputs)\n",
    "            logits = outputs.logits\n",
    "            probs = _torch.nn.functional.softmax(logits, dim=1).cpu().numpy()\n",
    "        cfg = getattr(finbert_model_obj, 'config', None)\n",
    "        id2label = {int(k): v for k, v in getattr(cfg, 'id2label', {}).items()} if cfg is not None else {}\n",
    "        for i, t in enumerate(texts):\n",
    "            print('\\nText:', t)\n",
    "            row = probs[i]\n",
    "            for idx, p in enumerate(row):\n",
    "                lbl = id2label.get(idx, str(idx))\n",
    "                print(f\"   {lbl}: {p:.4f}\")\n",
    "            # margin\n",
    "            import numpy as _np\n",
    "            top_idx = int(row.argmax())\n",
    "            second = _np.partition(row, -2)[-2:][0]\n",
    "            margin = float(row[top_idx] - second)\n",
    "            print('  Margin:', margin)\n",
    "    else:\n",
    "        # pipeline fallback\n",
    "        res = finbert(texts, return_all_scores=True)\n",
    "        for t, d in zip(texts, res):\n",
    "            print('\\nText:', t)\n",
    "            for entry in sorted(d, key=lambda x: x.get('label','')):\n",
    "                print(f\"   {entry.get('label')}: {entry.get('score'):.4f}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print('FinBERT diagnostic failed or FinBERT not available:', e)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CS373",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
