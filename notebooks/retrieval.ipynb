{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7504b82f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yahoo articles: 10\n",
      "Fetching Google News: 2016-01\n",
      "Fetching Google News: 2016-02\n",
      "Fetching Google News: 2016-03\n",
      "Fetching Google News: 2016-04\n",
      "Fetching Google News: 2016-05\n",
      "Fetching Google News: 2016-06\n",
      "Fetching Google News: 2016-07\n",
      "Fetching Google News: 2016-08\n",
      "Fetching Google News: 2016-09\n",
      "Fetching Google News: 2016-10\n",
      "Fetching Google News: 2016-11\n",
      "Fetching Google News: 2016-12\n",
      "Fetching Google News: 2017-01\n",
      "Fetching Google News: 2017-02\n",
      "Fetching Google News: 2017-03\n",
      "Fetching Google News: 2017-04\n",
      "Fetching Google News: 2017-05\n",
      "Fetching Google News: 2017-06\n",
      "Fetching Google News: 2017-07\n",
      "Fetching Google News: 2017-08\n",
      "Fetching Google News: 2017-09\n",
      "Fetching Google News: 2017-10\n",
      "Fetching Google News: 2017-11\n",
      "Fetching Google News: 2017-12\n",
      "Fetching Google News: 2018-01\n",
      "Fetching Google News: 2018-02\n",
      "Fetching Google News: 2018-03\n",
      "Fetching Google News: 2018-04\n",
      "Fetching Google News: 2018-05\n",
      "Fetching Google News: 2018-06\n",
      "Fetching Google News: 2018-07\n",
      "Fetching Google News: 2018-08\n",
      "Fetching Google News: 2018-09\n",
      "Fetching Google News: 2018-10\n",
      "Fetching Google News: 2018-11\n",
      "Fetching Google News: 2018-12\n",
      "Fetching Google News: 2019-01\n",
      "Fetching Google News: 2019-02\n",
      "Fetching Google News: 2019-03\n",
      "Fetching Google News: 2019-04\n",
      "Fetching Google News: 2019-05\n",
      "Fetching Google News: 2019-06\n",
      "Fetching Google News: 2019-07\n",
      "Fetching Google News: 2019-08\n",
      "Fetching Google News: 2019-09\n",
      "Fetching Google News: 2019-10\n",
      "Fetching Google News: 2019-11\n",
      "Fetching Google News: 2019-12\n",
      "Fetching Google News: 2020-01\n",
      "Fetching Google News: 2020-02\n",
      "Fetching Google News: 2020-03\n",
      "Fetching Google News: 2020-04\n",
      "Fetching Google News: 2020-05\n",
      "Fetching Google News: 2020-06\n",
      "Fetching Google News: 2020-07\n",
      "Fetching Google News: 2020-08\n",
      "Fetching Google News: 2020-09\n",
      "Fetching Google News: 2020-10\n",
      "Fetching Google News: 2020-11\n",
      "Fetching Google News: 2020-12\n",
      "Fetching Google News: 2021-01\n",
      "Fetching Google News: 2021-02\n",
      "Fetching Google News: 2021-03\n",
      "Fetching Google News: 2021-04\n",
      "Fetching Google News: 2021-05\n",
      "Fetching Google News: 2021-06\n",
      "Fetching Google News: 2021-07\n",
      "Fetching Google News: 2021-08\n",
      "Fetching Google News: 2021-09\n",
      "Fetching Google News: 2021-10\n",
      "Fetching Google News: 2021-11\n",
      "Fetching Google News: 2021-12\n",
      "Fetching Google News: 2022-01\n",
      "Fetching Google News: 2022-02\n",
      "Fetching Google News: 2022-03\n",
      "Fetching Google News: 2022-04\n",
      "Fetching Google News: 2022-05\n",
      "Fetching Google News: 2022-06\n",
      "Fetching Google News: 2022-07\n",
      "Fetching Google News: 2022-08\n",
      "Fetching Google News: 2022-09\n",
      "Fetching Google News: 2022-10\n",
      "Fetching Google News: 2022-11\n",
      "Fetching Google News: 2022-12\n",
      "Fetching Google News: 2023-01\n",
      "Fetching Google News: 2023-02\n",
      "Fetching Google News: 2023-03\n",
      "Fetching Google News: 2023-04\n",
      "Fetching Google News: 2023-05\n",
      "Fetching Google News: 2023-06\n",
      "Fetching Google News: 2023-07\n",
      "Fetching Google News: 2023-08\n",
      "Fetching Google News: 2023-09\n",
      "Fetching Google News: 2023-10\n",
      "Fetching Google News: 2023-11\n",
      "Fetching Google News: 2023-12\n",
      "Fetching Google News: 2024-01\n",
      "Fetching Google News: 2024-02\n",
      "Fetching Google News: 2024-03\n",
      "Fetching Google News: 2024-04\n",
      "Fetching Google News: 2024-05\n",
      "Fetching Google News: 2024-06\n",
      "Fetching Google News: 2024-07\n",
      "Fetching Google News: 2024-08\n",
      "Fetching Google News: 2024-09\n",
      "Fetching Google News: 2024-10\n",
      "Fetching Google News: 2024-11\n",
      "Fetching Google News: 2024-12\n",
      "Google articles (raw): 2139\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'sp500_features.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 130\u001b[39m\n\u001b[32m    125\u001b[39m df_all = df_all.drop(columns=[\u001b[33m\"\u001b[39m\u001b[33mtitle_norm\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m    127\u001b[39m \u001b[38;5;66;03m# =========================\u001b[39;00m\n\u001b[32m    128\u001b[39m \u001b[38;5;66;03m# 5. Align to AAPL trading days\u001b[39;00m\n\u001b[32m    129\u001b[39m \u001b[38;5;66;03m# =========================\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m market_df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msp500_features.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdate\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    132\u001b[39m aapl_trading_days = \u001b[38;5;28mset\u001b[39m(\n\u001b[32m    133\u001b[39m     market_df[market_df[\u001b[33m\"\u001b[39m\u001b[33mticker\u001b[39m\u001b[33m\"\u001b[39m] == TICKER][\u001b[33m\"\u001b[39m\u001b[33mdate\u001b[39m\u001b[33m\"\u001b[39m].unique()\n\u001b[32m    134\u001b[39m )\n\u001b[32m    136\u001b[39m df_all = df_all[df_all[\u001b[33m\"\u001b[39m\u001b[33mdate\u001b[39m\u001b[33m\"\u001b[39m].isin(aapl_trading_days)]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/rocky9/2024.09/CS373/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/rocky9/2024.09/CS373/lib/python3.11/site-packages/pandas/io/parsers/readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/rocky9/2024.09/CS373/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/rocky9/2024.09/CS373/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/rocky9/2024.09/CS373/lib/python3.11/site-packages/pandas/io/common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'sp500_features.csv'"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 0. Imports & config\n",
    "# =========================\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from gnews import GNews\n",
    "\n",
    "TICKER = \"AAPL\"\n",
    "START_YEAR = 2016\n",
    "END_YEAR   = 2024\n",
    "\n",
    "# =========================\n",
    "# 1. Yahoo Finance (recent only)\n",
    "# =========================\n",
    "import requests\n",
    "from datetime import datetime\n",
    "\n",
    "def fetch_yahoo_news_api(ticker, count=100):\n",
    "    url = (\n",
    "        f\"https://query1.finance.yahoo.com/v1/finance/search?\"\n",
    "        f\"q={ticker}&newsCount={count}&quotesCount=0\"\n",
    "    )\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "    r = requests.get(url, headers=headers).json()\n",
    "\n",
    "    if \"news\" not in r:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    rows = []\n",
    "    for item in r[\"news\"]:\n",
    "        try:\n",
    "            rows.append({\n",
    "                \"ticker\": ticker,\n",
    "                \"title\": item.get(\"title\"),\n",
    "                \"publisher\": item.get(\"publisher\"),\n",
    "                \"datetime\": datetime.utcfromtimestamp(item.get(\"providerPublishTime\")),\n",
    "                \"url\": item.get(\"link\"),\n",
    "                \"source\": \"yahoo\",\n",
    "            })\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "df_yahoo = fetch_yahoo_news_api(TICKER, count=100)\n",
    "\n",
    "print(\"Yahoo articles:\", len(df_yahoo))\n",
    "\n",
    "# =========================\n",
    "# 2. Google News (MONTHLY)\n",
    "# =========================\n",
    "def fetch_gnews_monthly(ticker, year, month):\n",
    "    google = GNews(\n",
    "        language=\"en\",\n",
    "        country=\"US\",\n",
    "        max_results=100\n",
    "    )\n",
    "\n",
    "    start_date = datetime(year, month, 1)\n",
    "    if month == 12:\n",
    "        end_date = datetime(year + 1, 1, 1)\n",
    "    else:\n",
    "        end_date = datetime(year, month + 1, 1)\n",
    "\n",
    "    google.start_date = start_date\n",
    "    google.end_date = end_date\n",
    "\n",
    "    query = f\"{ticker} stock\".replace(\" \", \"+\")\n",
    "    articles = google.get_news(query)\n",
    "\n",
    "    rows = []\n",
    "    for a in articles:\n",
    "        try:\n",
    "            dt = pd.to_datetime(a[\"published date\"])\n",
    "        except:\n",
    "            dt = None\n",
    "\n",
    "        rows.append({\n",
    "            \"ticker\": ticker,\n",
    "            \"title\": a.get(\"title\"),\n",
    "            \"publisher\": a.get(\"publisher\", {}).get(\"title\", \"\"),\n",
    "            \"datetime\": dt,\n",
    "            \"url\": a.get(\"url\"),\n",
    "            \"source\": \"google\",\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "dfs_google = []\n",
    "\n",
    "for year in range(START_YEAR, END_YEAR + 1):\n",
    "    for month in range(1, 13):\n",
    "        print(f\"Fetching Google News: {year}-{month:02d}\")\n",
    "        df_m = fetch_gnews_monthly(TICKER, year, month)\n",
    "        dfs_google.append(df_m)\n",
    "\n",
    "df_google = pd.concat(dfs_google, ignore_index=True)\n",
    "print(\"Google articles (raw):\", len(df_google))\n",
    "\n",
    "# =========================\n",
    "# 3. Combine Yahoo + Google\n",
    "# =========================\n",
    "df_all = pd.concat([df_google, df_yahoo], ignore_index=True)\n",
    "\n",
    "# Basic cleaning\n",
    "df_all[\"datetime\"] = pd.to_datetime(df_all[\"datetime\"], errors=\"coerce\")\n",
    "df_all = df_all.dropna(subset=[\"datetime\", \"title\"])\n",
    "\n",
    "# Daily key for LSTM alignment\n",
    "df_all[\"date\"] = df_all[\"datetime\"].dt.date\n",
    "df_all[\"date\"] = pd.to_datetime(df_all[\"date\"])\n",
    "\n",
    "# =========================\n",
    "# 4. Light deduplication\n",
    "# =========================\n",
    "df_all[\"title_norm\"] = (\n",
    "    df_all[\"title\"]\n",
    "    .str.lower()\n",
    "    .str.replace(r\"[^a-z0-9 ]\", \"\", regex=True)\n",
    "    .str.replace(r\"\\s+\", \" \", regex=True)\n",
    "    .str.strip()\n",
    ")\n",
    "\n",
    "df_all = df_all.drop_duplicates(subset=[\"title_norm\"])\n",
    "df_all = df_all.drop(columns=[\"title_norm\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf09f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================\n",
      "FINAL RESULT\n",
      "Total articles: 1849\n",
      "  ticker       date            datetime  \\\n",
      "0   AAPL 2016-01-06 2016-01-06 08:00:00   \n",
      "1   AAPL 2016-01-06 2016-01-06 08:00:00   \n",
      "2   AAPL 2016-01-08 2016-01-08 08:00:00   \n",
      "3   AAPL 2016-01-11 2016-01-11 08:00:00   \n",
      "4   AAPL 2016-01-12 2016-01-12 08:00:00   \n",
      "\n",
      "                                               title  \\\n",
      "0  Apple stock crash nears another scary level - ...   \n",
      "1  Apple Inc. ($AAPL) Stock | Crushed Premarket O...   \n",
      "2  Best Big Tech Stocks Of 2015 Can Ring In 2016 ...   \n",
      "3  Survey Suggests iPhone Sales Grew 33% in China...   \n",
      "4  How Tesla and Nissan’s Self-Parking Cars Fores...   \n",
      "\n",
      "                   publisher  \\\n",
      "0                  USA Today   \n",
      "1     warriortradingnews.com   \n",
      "2  Investor's Business Daily   \n",
      "3                    Fortune   \n",
      "4                    Fortune   \n",
      "\n",
      "                                                 url  source  \n",
      "0  https://news.google.com/rss/articles/CBMiigFBV...  google  \n",
      "1  https://news.google.com/rss/articles/CBMisgFBV...  google  \n",
      "2  https://news.google.com/rss/articles/CBMifEFVX...  google  \n",
      "3  https://news.google.com/rss/articles/CBMiZ0FVX...  google  \n",
      "4  https://news.google.com/rss/articles/CBMiaEFVX...  google  \n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 5. Align to AAPL trading days\n",
    "# =========================\n",
    "market_df = pd.read_csv(\"../data/sp500_historical_clean.csv\", parse_dates=[\"Date\"])\n",
    "\n",
    "aapl_trading_days = set(\n",
    "    market_df[market_df[\"Ticker\"] == TICKER][\"Date\"].unique()\n",
    ")\n",
    "\n",
    "df_all = df_all[df_all[\"date\"].isin(aapl_trading_days)]\n",
    "\n",
    "# =========================\n",
    "# 6. Final sort & save\n",
    "# =========================\n",
    "df_all = df_all.sort_values([\"date\", \"datetime\"]).reset_index(drop=True)\n",
    "\n",
    "df_all = df_all[[\n",
    "    \"ticker\",\n",
    "    \"date\",\n",
    "    \"datetime\",\n",
    "    \"title\",\n",
    "    \"publisher\",\n",
    "    \"url\",\n",
    "    \"source\"\n",
    "]]\n",
    "\n",
    "df_all.to_csv(\"../data/aapl_news_raw.csv\", index=False)\n",
    "\n",
    "print(\"=================================\")\n",
    "print(\"FINAL RESULT\")\n",
    "print(\"Total articles:\", len(df_all))\n",
    "print(df_all.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c5626d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pham156/.conda/envs/rocky9/2024.09/CS373/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "12/14/2025 04:58:25 PM - Use pytorch device_name: cuda:0\n",
      "12/14/2025 04:58:25 PM - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "Batches: 100%|██████████| 58/58 [00:07<00:00,  8.13it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "df = pd.read_csv(\"../data/aapl_news_raw.csv\", parse_dates=[\"date\", \"datetime\"])\n",
    "\n",
    "sbert = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "df[\"sbert_emb\"] = list(\n",
    "    sbert.encode(df[\"title\"].tolist(), show_progress_bar=True)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8ea973",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/14/2025 05:01:36 PM - Loading faiss with AVX512 support.\n",
      "12/14/2025 05:01:38 PM - Successfully loaded faiss with AVX512 support.\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  1.76it/s]\n",
      "Daily FAISS retrieval: 100%|██████████| 1010/1010 [00:02<00:00, 361.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved aapl_daily_retrieved_articles.csv\n",
      "Total retrieved rows: 10065\n",
      "  ticker       date    article_datetime  \\\n",
      "0   AAPL 2016-01-06 2016-01-06 08:00:00   \n",
      "1   AAPL 2016-01-06 2016-01-06 08:00:00   \n",
      "2   AAPL 2016-01-08 2016-01-06 08:00:00   \n",
      "3   AAPL 2016-01-08 2016-01-06 08:00:00   \n",
      "4   AAPL 2016-01-08 2016-01-08 08:00:00   \n",
      "\n",
      "                                               title  \\\n",
      "0  Apple Inc. ($AAPL) Stock | Crushed Premarket O...   \n",
      "1  Apple stock crash nears another scary level - ...   \n",
      "2  Apple Inc. ($AAPL) Stock | Crushed Premarket O...   \n",
      "3  Apple stock crash nears another scary level - ...   \n",
      "4  Best Big Tech Stocks Of 2015 Can Ring In 2016 ...   \n",
      "\n",
      "                   publisher  \\\n",
      "0     warriortradingnews.com   \n",
      "1                  USA Today   \n",
      "2     warriortradingnews.com   \n",
      "3                  USA Today   \n",
      "4  Investor's Business Daily   \n",
      "\n",
      "                                                 url  source  faiss_rank  \\\n",
      "0  https://news.google.com/rss/articles/CBMisgFBV...  google           1   \n",
      "1  https://news.google.com/rss/articles/CBMiigFBV...  google           2   \n",
      "2  https://news.google.com/rss/articles/CBMisgFBV...  google           1   \n",
      "3  https://news.google.com/rss/articles/CBMiigFBV...  google           2   \n",
      "4  https://news.google.com/rss/articles/CBMifEFVX...  google           3   \n",
      "\n",
      "   faiss_distance  \n",
      "0        0.646815  \n",
      "1        1.269917  \n",
      "2        0.646815  \n",
      "3        1.269917  \n",
      "4        1.305442  \n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "from tqdm import tqdm\n",
    "\n",
    "TICKER = \"AAPL\"\n",
    "TOP_K = 10\n",
    "QUERY_TEXT = \"AAPL financial news\"\n",
    "\n",
    "# Encode query ONCE\n",
    "query_emb = sbert.encode([QUERY_TEXT], convert_to_numpy=True).astype(\"float32\")\n",
    "EMB_DIM = query_emb.shape[1]\n",
    "\n",
    "results = []\n",
    "\n",
    "# Ensure sorted by date\n",
    "df = df.sort_values(\"date\").reset_index(drop=True)\n",
    "\n",
    "all_dates = df[\"date\"].unique()\n",
    "\n",
    "for d in tqdm(all_dates, desc=\"Daily FAISS retrieval\"):\n",
    "    # Use only articles up to day d\n",
    "    df_past = df[df[\"date\"] <= d]\n",
    "\n",
    "    if len(df_past) == 0:\n",
    "        continue\n",
    "\n",
    "    X = np.vstack(df_past[\"sbert_emb\"].values).astype(\"float32\")\n",
    "\n",
    "    index = faiss.IndexFlatL2(EMB_DIM)\n",
    "    index.add(X)\n",
    "\n",
    "    k = min(TOP_K, len(df_past))\n",
    "    distances, indices = index.search(query_emb, k)\n",
    "\n",
    "    for rank, (idx, dist) in enumerate(zip(indices[0], distances[0]), start=1):\n",
    "        row = df_past.iloc[idx]\n",
    "\n",
    "        results.append({\n",
    "            \"ticker\": TICKER,\n",
    "            \"date\": d,\n",
    "            \"article_datetime\": row[\"datetime\"],\n",
    "            \"title\": row[\"title\"],\n",
    "            \"publisher\": row.get(\"publisher\", \"\"),\n",
    "            \"url\": row.get(\"url\", \"\"),\n",
    "            \"source\": row.get(\"source\", \"\"),\n",
    "            \"faiss_rank\": rank,\n",
    "            \"faiss_distance\": float(dist),\n",
    "        })\n",
    "\n",
    "# Save retrieval-only dataset\n",
    "retrieved_df = pd.DataFrame(results)\n",
    "retrieved_df = retrieved_df.sort_values([\"date\", \"faiss_rank\"]).reset_index(drop=True)\n",
    "\n",
    "retrieved_df.to_csv(\"../data/aapl_daily_retrieved_articles.csv\", index=False)\n",
    "\n",
    "print(\"Saved aapl_daily_retrieved_articles.csv\")\n",
    "print(\"Total retrieved rows:\", len(retrieved_df))\n",
    "print(retrieved_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ffefffc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "DATA_DIR = \"../data\"\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "FEATURE_NAMES = [\n",
    "    \"polarity\", \"intensity\", \"relevance\", \"short_term\",\n",
    "    \"long_term\", \"volatility\", \"novelty\", \"credibility\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06045207",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_pool(vectors):\n",
    "    return np.mean(vectors, axis=0)\n",
    "\n",
    "def weighted_pool(vectors, distances=None, eps=1e-6):\n",
    "    vectors = np.array(vectors)\n",
    "\n",
    "    if distances is None:\n",
    "        weights = np.ones(len(vectors))\n",
    "    else:\n",
    "        similarities = 1.0 / (np.array(distances) + eps)\n",
    "        intensities = vectors[:, 1]  # intensity dimension\n",
    "        weights = similarities * (1.0 + intensities)\n",
    "\n",
    "    weights = weights / (weights.sum() + eps)\n",
    "    return (vectors * weights[:, None]).sum(axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1f9b7ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "12/14/2025 05:07:57 PM - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import json\n",
    "import re\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "EMBED_SYSTEM_PROMPT = \"\"\"\n",
    "You are a financial news embedding model. Your task is to convert financial text\n",
    "into a JSON object with EXACTLY 8 numeric features. Each value must be a float\n",
    "in the range [-1, 1].\n",
    "\n",
    "The 8 features are:\n",
    "1. polarity       (negative → positive sentiment)\n",
    "2. intensity      (strength of sentiment)\n",
    "3. relevance      (how directly it relates to TARGET_TICKER)\n",
    "4. short_term     (expected next-day price impact)\n",
    "5. long_term      (expected multi-week impact)\n",
    "6. volatility     (uncertainty implied by the text)\n",
    "7. novelty        (new vs repeated information)\n",
    "8. credibility    (rumor → fact-based)\n",
    "\n",
    "You MUST output only a JSON object with these 8 fields.\n",
    "\n",
    "--------------------------------------------------------\n",
    "FEW-SHOT EXAMPLES\n",
    "--------------------------------------------------------\n",
    "\n",
    "Example A:\n",
    "TEXT: \"Netflix subscriber growth slows as competition intensifies across streaming platforms.\"\n",
    "TARGET_TICKER: NFLX\n",
    "\n",
    "OUTPUT:\n",
    "{\n",
    "  \"polarity\": -0.50,\n",
    "  \"intensity\": 0.70,\n",
    "  \"relevance\": 0.95,\n",
    "  \"short_term\": -0.40,\n",
    "  \"long_term\": -0.20,\n",
    "  \"volatility\": 0.60,\n",
    "  \"novelty\": 0.55,\n",
    "  \"credibility\": 0.90\n",
    "}\n",
    "\n",
    "Example B:\n",
    "TEXT: \"Coca-Cola reports strong international sales and raises full-year outlook.\"\n",
    "TARGET_TICKER: KO\n",
    "\n",
    "OUTPUT:\n",
    "{\n",
    "  \"polarity\": 0.60,\n",
    "  \"intensity\": 0.55,\n",
    "  \"relevance\": 0.90,\n",
    "  \"short_term\": 0.35,\n",
    "  \"long_term\": 0.50,\n",
    "  \"volatility\": 0.20,\n",
    "  \"novelty\": 0.30,\n",
    "  \"credibility\": 0.95\n",
    "}\n",
    "\n",
    "Example C:\n",
    "TEXT: \"Boeing receives a major multi-billion dollar aircraft order from a Middle Eastern airline.\"\n",
    "TARGET_TICKER: BA\n",
    "\n",
    "OUTPUT:\n",
    "{\n",
    "  \"polarity\": 0.80,\n",
    "  \"intensity\": 0.85,\n",
    "  \"relevance\": 1.00,\n",
    "  \"short_term\": 0.70,\n",
    "  \"long_term\": 0.75,\n",
    "  \"volatility\": 0.40,\n",
    "  \"novelty\": 0.70,\n",
    "  \"credibility\": 0.95\n",
    "}\n",
    "\n",
    "Example D:\n",
    "TEXT: \"Meta Faces a global outage across Instagram and Facebook services.\"\n",
    "TARGET_TICKER: META\n",
    "\n",
    "OUTPUT:\n",
    "{\n",
    "  \"polarity\": -0.75,\n",
    "  \"intensity\": 0.80,\n",
    "  \"relevance\": 1.00,\n",
    "  \"short_term\": -0.60,\n",
    "  \"long_term\": -0.35,\n",
    "  \"volatility\": 0.85,\n",
    "  \"novelty\": 0.65,\n",
    "  \"credibility\": 0.90\n",
    "}\n",
    "\n",
    "Example E:\n",
    "TEXT: \"Oil prices rise as OPEC announces unexpected production cuts.\"\n",
    "TARGET_TICKER: XLE\n",
    "\n",
    "OUTPUT:\n",
    "{\n",
    "  \"polarity\": 0.30,\n",
    "  \"intensity\": 0.65,\n",
    "  \"relevance\": 0.85,\n",
    "  \"short_term\": 0.40,\n",
    "  \"long_term\": 0.25,\n",
    "  \"volatility\": 0.70,\n",
    "  \"novelty\": 0.50,\n",
    "  \"credibility\": 0.95\n",
    "}\n",
    "\n",
    "--------------------------------------------------------\n",
    "\n",
    "NOW PROCESS THE NEW INPUT.\n",
    "Return ONLY the JSON object.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def generate_embedding(text, ticker=\"AAPL\"):\n",
    "    user_prompt = f\"\"\"\n",
    "TARGET_TICKER: {ticker}\n",
    "\n",
    "TEXT:\n",
    "{text}\n",
    "\n",
    "Return ONLY the JSON object with the 8 fields.\n",
    "\"\"\"\n",
    "\n",
    "    prompt = f\"<|system|>\\n{EMBED_SYSTEM_PROMPT}\\n<|user|>\\n{user_prompt}\\n<|assistant|>\\n\"\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=300,\n",
    "        temperature=0.0,\n",
    "        do_sample=False\n",
    "    )\n",
    "\n",
    "    decoded = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    # extract ALL JSON objects\n",
    "    matches = re.findall(r\"\\{.*?\\}\", decoded, flags=re.DOTALL)\n",
    "\n",
    "    if not matches:\n",
    "        print(\"No JSON found:\\n\", decoded)\n",
    "        return None\n",
    "\n",
    "    # use the LAST JSON (assistant output)\n",
    "    json_str = matches[-1]\n",
    "\n",
    "    try:\n",
    "        obj = json.loads(json_str)\n",
    "\n",
    "        vector = [\n",
    "            obj[\"polarity\"],\n",
    "            obj[\"intensity\"],\n",
    "            obj[\"relevance\"],\n",
    "            obj[\"short_term\"],\n",
    "            obj[\"long_term\"],\n",
    "            obj[\"volatility\"],\n",
    "            obj[\"novelty\"],\n",
    "            obj[\"credibility\"]\n",
    "        ]\n",
    "\n",
    "        return vector\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"JSON parse error:\", e)\n",
    "        print(\"Raw JSON:\", json_str)\n",
    "        print(\"Full Output:\", decoded)\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "017e0df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = pd.read_csv(\"../data/aapl_news_raw.csv\", parse_dates=[\"date\"])\n",
    "df_raw = df_raw.sort_values(\"date\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cba0ae30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RAW aggregation:   0%|          | 0/1010 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "RAW aggregation: 100%|██████████| 1010/1010 [1:34:08<00:00,  5.59s/it] \n"
     ]
    }
   ],
   "source": [
    "rows_mean = []\n",
    "rows_weighted = []\n",
    "\n",
    "for d, group in tqdm(df_raw.groupby(\"date\"), desc=\"RAW aggregation\"):\n",
    "    vectors = []\n",
    "\n",
    "    for title in group[\"title\"]:\n",
    "        v = generate_embedding(title, ticker=\"AAPL\")\n",
    "        if v is not None:\n",
    "            vectors.append(v)\n",
    "\n",
    "    if len(vectors) == 0:\n",
    "        agg_mean = np.zeros(8)\n",
    "        agg_weighted = np.zeros(8)\n",
    "    else:\n",
    "        agg_mean = mean_pool(vectors)\n",
    "        agg_weighted = weighted_pool(vectors)\n",
    "\n",
    "    rows_mean.append([\"AAPL\", d] + agg_mean.tolist())\n",
    "    rows_weighted.append([\"AAPL\", d] + agg_weighted.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca100faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"ticker\", \"date\"] + FEATURE_NAMES\n",
    "\n",
    "pd.DataFrame(rows_mean, columns=cols).to_csv(\n",
    "    f\"{DATA_DIR}/aapl_daily_sentiment_raw_mean.csv\", index=False\n",
    ")\n",
    "\n",
    "pd.DataFrame(rows_weighted, columns=cols).to_csv(\n",
    "    f\"{DATA_DIR}/aapl_daily_sentiment_raw_weighted.csv\", index=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d40b4a4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding 1849 unique articles\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Qwen sentiment encoding: 100%|██████████| 1849/1849 [1:30:57<00:00,  2.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================\n",
      "Saved ../data/aapl_article_sentiment.csv\n",
      "Total encoded articles: 1849\n",
      "  ticker                                              title  polarity  \\\n",
      "0   AAPL  Apple stock crash nears another scary level - ...      -0.8   \n",
      "1   AAPL  Apple Inc. ($AAPL) Stock | Crushed Premarket O...      -0.9   \n",
      "2   AAPL  Best Big Tech Stocks Of 2015 Can Ring In 2016 ...       0.5   \n",
      "3   AAPL  Survey Suggests iPhone Sales Grew 33% in China...       0.5   \n",
      "4   AAPL  How Tesla and Nissan’s Self-Parking Cars Fores...       0.5   \n",
      "\n",
      "   intensity  relevance  short_term  long_term  volatility  novelty  \\\n",
      "0       0.85       0.95        -0.7       -0.5        0.65     0.55   \n",
      "1       0.85       0.95        -0.7       -0.5        0.65     0.80   \n",
      "2       0.70       0.95         0.3        0.2        0.60     0.55   \n",
      "3       0.70       0.95         0.3        0.2        0.60     0.55   \n",
      "4       0.70       0.95         0.3        0.2        0.60     0.55   \n",
      "\n",
      "   credibility  \n",
      "0         0.90  \n",
      "1         0.95  \n",
      "2         0.90  \n",
      "3         0.90  \n",
      "4         0.90  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# Article-level Sentiment Extraction (ONCE)\n",
    "# =========================================\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Load raw news\n",
    "df_raw = pd.read_csv(\"../data/aapl_news_raw.csv\")\n",
    "\n",
    "FEATURES = [\n",
    "    \"polarity\", \"intensity\", \"relevance\", \"short_term\",\n",
    "    \"long_term\", \"volatility\", \"novelty\", \"credibility\"\n",
    "]\n",
    "\n",
    "embedding_cache = {}\n",
    "rows = []\n",
    "\n",
    "unique_titles = df_raw[\"title\"].dropna().unique()\n",
    "print(f\"Encoding {len(unique_titles)} unique articles\")\n",
    "\n",
    "for title in tqdm(unique_titles, desc=\"Qwen sentiment encoding\"):\n",
    "    try:\n",
    "        v = generate_embedding(title, ticker=\"AAPL\")\n",
    "        if v is None:\n",
    "            continue\n",
    "\n",
    "        embedding_cache[title] = v\n",
    "        rows.append([\"AAPL\", title] + v)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error on title:\", title)\n",
    "        print(e)\n",
    "\n",
    "# Save article-level sentiment\n",
    "cols = [\"ticker\", \"title\"] + FEATURES\n",
    "df_article_sent = pd.DataFrame(rows, columns=cols)\n",
    "\n",
    "df_article_sent.to_csv(\n",
    "    \"../data/aapl_article_sentiment.csv\",\n",
    "    index=False\n",
    ")\n",
    "\n",
    "print(\"=================================\")\n",
    "print(\"Saved ../data/aapl_article_sentiment.csv\")\n",
    "print(\"Total encoded articles:\", len(df_article_sent))\n",
    "print(df_article_sent.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6224e33a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FAISS aggregation: 100%|██████████| 1010/1010 [00:00<00:00, 2622.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS sentiment aggregation DONE.\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# FAST FAISS Sentiment Aggregation (POST-QWEN)\n",
    "# =========================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "FEATURES = [\n",
    "    \"polarity\", \"intensity\", \"relevance\", \"short_term\",\n",
    "    \"long_term\", \"volatility\", \"novelty\", \"credibility\"\n",
    "]\n",
    "\n",
    "# Load FAISS retrieval\n",
    "df_faiss = pd.read_csv(\n",
    "    \"../data/aapl_daily_retrieved_articles.csv\",\n",
    "    parse_dates=[\"date\", \"article_datetime\"]\n",
    ")\n",
    "\n",
    "# Load article-level sentiment (generated by Qwen)\n",
    "df_sent = pd.read_csv(\"../data/aapl_article_sentiment.csv\")\n",
    "\n",
    "# Merge sentiment onto FAISS rows\n",
    "df_faiss = df_faiss.merge(df_sent, on=\"title\", how=\"left\")\n",
    "\n",
    "def mean_pool(v):\n",
    "    return v.mean(axis=0)\n",
    "\n",
    "def weighted_pool(v, d, eps=1e-6):\n",
    "    w = (1.0 / (d + eps)) * (1.0 + v[:, 1])  # intensity-aware\n",
    "    w = w / w.sum()\n",
    "    return (v * w[:, None]).sum(axis=0)\n",
    "\n",
    "rows_mean = []\n",
    "rows_weighted = []\n",
    "\n",
    "for d, group in tqdm(df_faiss.groupby(\"date\"), desc=\"FAISS aggregation\"):\n",
    "    vecs = group[FEATURES].values\n",
    "    dists = group[\"faiss_distance\"].values\n",
    "\n",
    "    if len(vecs) == 0:\n",
    "        agg_mean = np.zeros(8)\n",
    "        agg_weighted = np.zeros(8)\n",
    "    else:\n",
    "        agg_mean = mean_pool(vecs)\n",
    "        agg_weighted = weighted_pool(vecs, dists)\n",
    "\n",
    "    rows_mean.append([\"AAPL\", d] + agg_mean.tolist())\n",
    "    rows_weighted.append([\"AAPL\", d] + agg_weighted.tolist())\n",
    "\n",
    "cols = [\"ticker\", \"date\"] + FEATURES\n",
    "\n",
    "pd.DataFrame(rows_mean, columns=cols).to_csv(\n",
    "    \"../data/aapl_daily_sentiment_faiss_mean.csv\",\n",
    "    index=False\n",
    ")\n",
    "\n",
    "pd.DataFrame(rows_weighted, columns=cols).to_csv(\n",
    "    \"../data/aapl_daily_sentiment_faiss_weighted.csv\",\n",
    "    index=False\n",
    ")\n",
    "\n",
    "print(\"FAISS sentiment aggregation DONE.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CS373",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
